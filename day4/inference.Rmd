---
title: "Inference"
author: "Shammunul Islam"
date: "September 30, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Inference

"Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution."

Before understanding inference, we need to understand population and sample.

#### Population

A set of similar items or events which of interest for some question or experiment.

#### Sample

A subset of population using which we answer the question of interest.

In making inference, we first make assumption, then we take sample data and analyze. 

The first step of inference is making assumption or making hypothesis. There are two types of hypothesis.

* Null hypothesis (H0): There is no effect/there is no association. A statement which we are not interested in.

* Alternative hypothesis (HA): The effect/the relationship or the statement we are interested in to investigate.



### Testing change in proportion

```{r}
library(dplyr)
val = data.frame(
  admission = c(rep("admitted", 35), rep("not_admitteted", 13)),
              race = c(rep("white", 21), rep("black", 14), rep("white", 3), rep("black", 10)))
val %>% 
  group_by(race) %>%
  summarize(admittedd_prop = mean(admission == "admitted"))
#head(val)
```


The difference in proportion is approximately 0.29


```{r}
rep_sample_n <- function(tbl, size, replace = FALSE, reps = 1)
{
    n <- nrow(tbl)
    i <- unlist(replicate(reps, sample.int(n, size, replace = replace), 
                          simplify = FALSE))

    rep_tbl <- cbind(replicate = rep(1:reps,rep(size,reps)), tbl[i,])

    dplyr::group_by(rep_tbl, replicate)
}
# Sample the entire data frame 5 times
val %>%
  rep_sample_n(size = nrow(val), reps = 5) 
```

```{r}
# Shuffle the admission variable within replicate. 
val %>%
  rep_sample_n(size = nrow(val), reps = 5) %>%
  mutate(adm_perm = sample(admission)) 
```


Using rep_sample_n(), we do 1000 repeated samples (i.e. Replications) of the val data, then shuffled them using sample() to break any links between race and admission. Then for each replication, we calculate the proportions of admitted whites and blacks in the dataset along with the difference in proportions.

```{r}
# Create data frame of differences in admission rates
val_perm <- val %>%
  rep_sample_n(size = nrow(val), reps = 1000) %>%
  mutate(adm_perm = sample(admission))  %>%
  group_by(replicate, race) %>%
  summarize(prop_adm_perm = mean(adm_perm == "admitted"),
            prop_adm = mean(admission == "admitted")) %>%
  summarize(diff_perm = diff(prop_adm_perm),
            diff_orig = diff(prop_adm))  # white - black

library(ggplot2)
# Histogram of permuted differences
ggplot(val_perm, aes(x = diff_perm)) + 
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = diff_orig), col = "red")
```

Number of permuted difference greater than observed difference.

```{r}
(difference = val_perm %>%
  summarize(sum(diff_perm >= diff_orig)) %>% pull())
```

This difference (996) is how much of the total number of permutted differences.

```{r}
(difference / nrow(val_perm))*100
```

That means only 2.2% of the null distribution has value greter than our observed difference of 0.29.

### Using density

```{r}
ggplot(val_perm, aes(x = diff_perm)) + 
  geom_density() +
  geom_vline(aes(xintercept = diff_orig), col = "red")
```


### Quantile measurement

```{r}
val_perm %>%
  summarize(q.05 = quantile(diff_perm, p = 0.05),
            q0.95 = quantile(diff_perm, p = 0.95))
```

We see that observed difference  of 0.29 is higher than 95% quantile of the null distribution indicating that htere is actually a difference in admission rate between races.


### Rejection region or critical region -- the region where if our observed statistics fall we reject the null hypothesis

### P-value -- the probability of observing statistic as or more extreme than what we got in the null distribution.

#### Can you figure out the p-value for val data for our case at hand?

### 2-sided p-value

In the previous example, if we were only interested to if there was any difference in admission due to race, we allow for the diferences to be positive sometimes and negative sometimes. Now, just due to random chance this differnce could sometimes be be positive and sometimes negative. Thus, we now need to consider both tail sides of the distribution. This will double the size of the p-value for two-side test than the p-value for one-side test.

#### p-value for 2-sided test of any difference in admission between races:
 
```{r}
(difference = val_perm %>%
  summarize(2 * mean(diff_perm >= diff_orig)) %>% pull())
```

Normally, 5% is taken as standard p-value but we can also consider 1%. 

From the above result, we can conclude that it is not just the random variability that led to higher admission among white people. 
#### Remember that based on significance, we can't just say that race caused this difference in admission. If we want to establish a causal relation, we have to randomize our study.

### Now, we will study a case where some clients (50%) of Shwapno were chosen to be target for online advertising campaign (exclusive offers or as such) awareness program and some (50%) were not. All of these clients were grouped into similar class based on their purchase history. Now, after the campaign, we want to investigate whether this campaign had any positive effect in increasing purchase.

* H0: Online campaign doesn't have any influence on additional purchase.

* HA: Online campaign doesn't have any influence on additional purchase.

```{r}
promotions = read.csv("E:/Training_S/Day 4/promotion.csv")
str(promotions)
```

#### Base R approach to tabulation

```{r}
table(promotions$Promotion, promotions$Additional.Purchase)
```

#### dplyr approach to tabulation

```{r}
promotions %>%
  select(Promotion, Additional.Purchase) %>%
  table()
```

#### Find the proportion of people who had additional purchase within each group (who got promotion and who didn't)

```{r}
promotions %>%
  group_by(Promotion) %>%
  summarize(additional_prop = mean(Additional.Purchase == "More purchase"))
```

Let's create a barplot of purchase decision broken down by whether they were targetted for promotion or not.  

```{r}
library(RColorBrewer)
manual_fill = c("#5F9EA0", "#E1B378")
ggplot(promotions, aes(x=Promotion, fill = Additional.Purchase)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = manual_fill)
```

```{r}
prom_perm <- promotions %>%
  rep_sample_n(size = nrow(promotions), reps = 1000) %>%
  mutate(additional_perm = sample(Additional.Purchase)) %>%
  group_by(replicate, Promotion) %>%
  summarize(prop_purchase_perm = mean(additional_perm == "More purchase"),
            prop_purchase_orig = mean(Additional.Purchase == "More purchase")) %>%
  summarize(diff_perm = diff(prop_purchase_perm),
            diff_orig = diff(prop_purchase_orig))  # treatment - control


# Histogram of permuted differences
ggplot(prom_perm, aes(x = diff_perm)) +
  geom_histogram(binwidth = .005) +
  geom_vline(aes(xintercept = diff_orig), col = "red")
```

Now, we will check to see if this observed difference in proportion is consistent with the null difference.

```{r}
prom_perm %>%
  summarize(mean(diff_perm >= diff_orig))
```

### Confusion matrix

![](C:/Users/ERI/infer_1.png)

### Parameter vs. statistics

Parameter is a numerical value of some characteristics of the population. Statistics is a measure of this parameter using representative sample from the population.

### Standard error (SE)

SE of a statistic is the standard deviation of its sampling distribution.

### Confidence interval (CI)

Range of numbers that we hope with some confidence to represent the true parameter.

### Bootstrap

This let's us measure how far the statistic is from the parameter by taking repeated sample with replacement from the sample and computing statistic for each of these samples. This gives us a bootstrap distribution. Using bootstrap we can get an approximation of the standard error.

### Empirical rule

Approximately 95% of samples will produce statistics that are within 2 SE of the center.

### 2-types of CI

* Bootstrap CI using empirical rule
* Percentile interval

### Using infer package

### Now, we will focus on satifaction of customers. Let's understand the proportion of satisfied customers.

#### Observed proportion

```{r}
(observed_proportion = promotions %>%
  summarize(mean(Satisfaction=="Happy")) %>%
  pull())
```

#### Bootstrap distribution of proportion

```{r}
library(infer)
bootstrap = promotions %>%
  specify(response = Satisfaction, success = "Happy") %>%
  generate(reps = 300, type = "bootstrap") %>%
  calculate(stat = "prop")
bootstrap
```

Density plot of the bootstrap distribution

```{r}
ggplot(bootstrap, aes(x=stat)) +
  geom_density()
```

Standard deviation of this proportion

```{r}
(SE = bootstrap %>%
  summarize(sd(stat)) %>%
  pull())
```

#### Confidence interval

```{r}
c(observed_proportion - 2 * SE, observed_proportion + 2 * SE)
```

We are 95% confident that the proportion of all customers who are happy is between 0.54 and 0.67.

### Normal distribution for estimating SE

If observations are independent and n is large where n*p >= 10 and n(1-p) >= 10 then p follows a normal distribution where SE = root over of (p(1-p)/n). Here, p is p hat or estimation from the sample.

Let's check whether the condition for applying normal approximation is met for observed_proportion.

```{r}
n = nrow(promotions)
c(n * observed_proportion, n * (1 - observed_proportion))
```

As both are above 10, we can safely use normal approximation for calculating SE.

```{r}
(se_normal = sqrt(observed_proportion * (1 - observed_proportion) / n))
```

And bootstrap method gave us

```{r}
bootstrap %>% 
  summarize(sd(stat)) %>%
  pull()
```

We can see that both of these two SE estimates are very close.

Now, let's lay the sampling distribution of normal distribution of proportion on top of the bootstrap distribution.

```{r}
ggplot(bootstrap, aes(x = stat)) + 
  geom_density() +
  stat_function(fun = dnorm, color = "purple",
                args = list(mean = observed_proportion, sd = se_normal))
```

#### Hypothesis test for a particular value of proportion

Now, suppose we want to check whether the claim of 65% of the customers are happy are true or not.

```{r}
null_distribution = promotions %>%
  specify(response = Satisfaction, success = "Happy") %>%
  hypothesize(null = "point", p = 0.65) %>%
  generate(reps = 500, type = "simulate") %>%
  calculate(stat="prop")
```

How the observed proportion compares to null distribution?

```{r}
ggplot(null_distribution, aes(x=stat)) +
  geom_density() +
  geom_vline(xintercept = observed_proportion, col = "darkred")
```

#### Computing p-value

```{r}
null_distribution %>%
  summarize(mean(stat < observed_proportion)) %>%
  pull() * 2
```

As the p-value is 0.19, we can't reject the null hypothesis and we can say that the proportion of satisfied customer is 65%.

### Difference in proportion -- again

Now, let's go back to the example of admission by race. Now, we will use the admission data by race that we worked on before. But now, we will be using infer package.

```{r}
null_dist = promotions %>%
  specify(Additional.Purchase ~ Promotion, success = "More purchase") %>%
  hypothesize(null ="independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "diff in props", order = c("No", "Yes"))
```

#### Calculate p hat

```{r}
prop = promotions %>%
  group_by(Promotion) %>%
  summarize(mean(Additional.Purchase == "More purchase")) %>%
  pull()
```

#### Now calculate the observed difeence in proportion

```{r}
(diff_prop = diff(prop))
```

Let's again plot the observed difference on the null distribution:

```{r}
ggplot(null_dist, aes(x=stat)) +
  geom_density() +
  geom_vline(xintercept = diff_prop, col = "red")
```

### P-value

```{r}
null_dist %>%
  summarize(mean(stat > diff_prop)) %>%
  pull() * 2
```

### Comparing many categorical parameters

Is the geographical location associated with opinion towards freedom of press?

```{r}
opinion = read.csv("E:/Training_S/Day 4/opinion.csv")
str(opinion)
```

Let's explore this visually:

```{r}
ggplot(opinion, aes(Location, fill=Freedom)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = brewer.pal(8, "Set2")[c(1:3, 7)])
```

Let's tabulate it now:

```{r}
(observed = opinion %>%
  table())
```

Now, if the variables are independent of one another, we would have a null table of the counts of these observations, that is the table we would expect if the nvariables were independent of one annother. The total of squared difference between the observed cell count of the tabulation and the expected tabulation is called Chi-squared statistic. 

We can compute the chi-squared statistic in R in the following way:

```{r}
(observed_chi = chisq.test(observed)$statistic)
```

#### Generate null distribution for chi-square

```{r}
null_dist = opinion %>%
  specify(Freedom ~ Location) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "Chisq")
```

#### Plot observed Chi-square and the null distribution of Chi-square

```{r}
ggplot(null_dist, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = observed_chi, color = "blue") +
  stat_function(fun = dchisq, args = list(df = 6), color = "darkred")
```

#### P-value using permutation

```{r}
null_dist %>% 
  summarize(mean(stat > observed_chi)) %>% 
  pull()
```

#### P-value using approximation

Degree of freedom = (number of row - 1) * (number of column - 1)

```{r}
1 - pchisq(observed_chi, df = 6)
```

Chi-squared distribution is a good approximation when expected count in each cell is greater than 5 and df is greater than or equal to 2.

### Inference for numerical data

#### Calculate confidence interval of mean

```{r}
income = read.csv("E:/Training_S/Day 4/income.csv")
str(income)
```

#### Generate null distribution of mean income

```{r}
mean_income_null = income %>%
  specify(response = Income) %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "mean")
```

#### Observed mean

```{r}
obs_mean = income %>%
  summarize(mean(Income)) %>%
  pull()
```

#### Percentile method for CI

```{r}
mean_income_null %>%
  summarize(quantile(stat, 0.025),
            quantile(stat, 0.975))
```

#### Standard error method for CI

sample statistic +/- t * standard error of bootstrap distribution

![](C:/Users/ERI/infer_2.png)

#### Critical value of t

```{r}
t_critical = qt(0.975, df = nrow(income) - 1)
```

#### CI using SE

```{r}
mean_income_null %>%
  summarize(null_se = sd(stat)) %>%
  summarize(low = obs_mean - t_critical * null_se,
            high = obs_mean + t_critical * null_se)
```

#### Can you do it for median?


#### Someone claims that the average income is 120000, test it.

```{r}
point_income_null = income %>%
  specify(response = Income) %>%
  hypothesize(null = "point", mu = 120000) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

```{r}
# Calculate observed mean
mean_obs_inc <- income %>%
  summarize(mean(Income)) %>%
  pull()

# Calculate p-value
point_income_null %>%
  filter(stat > mean_obs_inc) %>%
  summarize(n() / 1000)
```

















